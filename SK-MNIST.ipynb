{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXynxrMQYZ30"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5435,
     "status": "ok",
     "timestamp": 1742156453432,
     "user": {
      "displayName": "Dimitri Caputo",
      "userId": "12075933984764029925"
     },
     "user_tz": -60
    },
    "id": "DOD-qHOuYZ33"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Rescaling, Dropout, Resizing\n",
    "from keras.layers import RandomFlip, RandomTranslation, RandomRotation, RandomZoom\n",
    "from keras.models import Sequential\n",
    "from keras.metrics import F1Score, Precision\n",
    "from keras.losses import CategoricalFocalCrossentropy, BinaryFocalCrossentropy\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, KMeansSMOTE\n",
    "from imblearn.under_sampling import TomekLinks, RepeatedEditedNearestNeighbours\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from sklearn.utils import compute_class_weight\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxVKOcXMYZ38"
   },
   "source": [
    "# Move images into class folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJQRTSqjYZ4a"
   },
   "outputs": [],
   "source": [
    "def move_and_separate_images(list_of_folders, df_metadata):\n",
    "    df_metadata = df_metadata.set_index('image_id', drop=True)\n",
    "    count = 0\n",
    "    for folder in list_of_folders:\n",
    "        for root, dirs, files in os.walk(folder):\n",
    "            for file in files:\n",
    "                try:\n",
    "                    os.makedirs(f\"dataset/{df_metadata.loc[file.strip('.jpg'), 'dx']}\")\n",
    "                except:\n",
    "                    source=os.path.join(root, file)\n",
    "                    destination=os.path.join('dataset', df_metadata.loc[file.strip('.jpg'), 'dx'], file)\n",
    "                    os.rename(source,destination)\n",
    "                    count += 1\n",
    "                    if count%100 == 0:\n",
    "                        print(f'{count} images were processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMuxfLJ-YZ4b"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('HAM10000_metadata.csv')\n",
    "\n",
    "move_and_separate_images(['HAM10000_images_part_1', 'HAM10000_images_part_2'], df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move images into cancerous/non cancerous folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapping = {\n",
    "    \"akiec\": \"canc\",\n",
    "    \"bcc\": \"canc\",\n",
    "    \"bkl\": \"nocanc\",\n",
    "    \"df\": \"nocanc\",\n",
    "    \"mel\": \"canc\",\n",
    "    \"nv\": \"nocanc\",\n",
    "    \"vasc\": \"nocanc\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_and_separate_images(list_of_folders, df_metadata):\n",
    "    df_metadata = df_metadata.set_index('image_id', drop=True)\n",
    "    count = 0\n",
    "    for folder in list_of_folders:\n",
    "        for root, dirs, files in os.walk(folder):\n",
    "            for file in files:\n",
    "                try:\n",
    "                    os.makedirs(f\"dataset/{class_mapping[df_metadata.loc[file.strip('.jpg'), 'dx']]}\")\n",
    "                except:\n",
    "                    source=os.path.join(root, file)\n",
    "                    destination=os.path.join('dataset', class_mapping[df_metadata.loc[file.strip('.jpg'), 'dx']], file)\n",
    "                    os.rename(source,destination)\n",
    "                    count += 1\n",
    "                    if count%100 == 0:\n",
    "                        print(f'{count} images were processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('HAM10000_metadata.csv')\n",
    "\n",
    "move_and_separate_images(['HAM10000_images_part_1', 'HAM10000_images_part_2'], df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMe2KXHYYZ3-"
   },
   "outputs": [],
   "source": [
    "def array_from_images(folder, df_metadata, dict_of_labels, h=224, w=224, channels=3):\n",
    "    # Create an array of images and labels the size of the number of pictures\n",
    "    nb_files = 0\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            nb_files += 1\n",
    "    array = np.zeros(shape=(nb_files, h, w, channels))\n",
    "    labels = np.zeros(shape=(nb_files,))\n",
    "\n",
    "    # Check the name and fill array and labels\n",
    "    df_metadata = df_metadata.set_index('image_id', drop=True)\n",
    "    count = 0\n",
    "\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            with Image.open(os.path.join(root, file)) as im:\n",
    "                array[count,:,:,:] = np.asarray(im.resize((h,w)))\n",
    "                try:\n",
    "                    labels[count] = dict_of_labels[class_mapping[df_metadata.loc[file.strip('.jpg'), 'dx']]]\n",
    "                except:\n",
    "                    labels[count] = dict_of_labels[df_metadata.loc[file.strip('.jpg'), 'dx']]\n",
    "                count += 1\n",
    "                if count%1000 == 0:\n",
    "                    print(f'{count} images were processed')\n",
    "    return array, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "wXfmnnH1YZ3_",
    "outputId": "caf9c7d6-299d-4569-cbc9-647f005a16d8"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('HAM10000_metadata.csv')\n",
    "dict_label = {k:v for k,v in zip(['nocanc', 'canc'], range(7))}\n",
    "\n",
    "size = 64\n",
    "\n",
    "X, y = array_from_images('dataset/', df, dict_label, h=size, w=size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(f'X-{size}x{size}', X, allow_pickle=True)\n",
    "np.savez_compressed(f'y-{size}x{size}', y, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1742156799969,
     "user": {
      "displayName": "Dimitri Caputo",
      "userId": "12075933984764029925"
     },
     "user_tz": -60
    },
    "id": "XPerG9OeY6Bm"
   },
   "outputs": [],
   "source": [
    "X = np.load('X-64x64.npz')['arr_0']\n",
    "y = np.load('y-64x64.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1044,
     "status": "ok",
     "timestamp": 1742156874055,
     "user": {
      "displayName": "Dimitri Caputo",
      "userId": "12075933984764029925"
     },
     "user_tz": -60
    },
    "id": "YQWIcLFUYZ4B"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, stratify=y, test_size=0.2, random_state=38)\n",
    "\n",
    "\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, stratify=y_temp, test_size=0.5, random_state=38)\n",
    "\n",
    "del X_temp, y_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wy2K0zqVYZ4N"
   },
   "source": [
    "# SMOTETOMEK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 855220,
     "status": "ok",
     "timestamp": 1741377156881,
     "user": {
      "displayName": "Dimitri Caputo",
      "userId": "12075933984764029925"
     },
     "user_tz": -60
    },
    "id": "I5f-Z8aOYZ4O",
    "outputId": "aa74acca-cf2d-4743-b399-9142749c28df"
   },
   "outputs": [],
   "source": [
    "shape_origin = X_train.shape\n",
    "\n",
    "X_train = np.reshape(X_train, (shape_origin[0], shape_origin[1]*shape_origin[2]*shape_origin[3]))\n",
    "\n",
    "smotetomek = SMOTETomek(random_state=38, n_jobs=-1)\n",
    "\n",
    "X_train, y_train = smotetomek.fit_resample(X_train, y_train)\n",
    "\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], shape_origin[1], shape_origin[2], shape_origin[3]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eugcgC1Ihbe7"
   },
   "source": [
    "# One Hot Encoding of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1742156903324,
     "user": {
      "displayName": "Dimitri Caputo",
      "userId": "12075933984764029925"
     },
     "user_tz": -60
    },
    "id": "GZnGDhBaYZ4V"
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hofZJHSSYZ4d"
   },
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1742156960318,
     "user": {
      "displayName": "Dimitri Caputo",
      "userId": "12075933984764029925"
     },
     "user_tz": -60
    },
    "id": "9ZlCHMEhYZ4e"
   },
   "outputs": [],
   "source": [
    "def get_earlystopping(patience=10):\n",
    "    early_stopping = EarlyStopping(\n",
    "    monitor='val_f1_score',\n",
    "    patience=patience,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True)\n",
    "    return early_stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1742156960330,
     "user": {
      "displayName": "Dimitri Caputo",
      "userId": "12075933984764029925"
     },
     "user_tz": -60
    },
    "id": "-cFyIbR9YZ4f"
   },
   "outputs": [],
   "source": [
    "def plot_learning_curves(model):\n",
    "    fig, ax = plt.subplots(1,3, figsize=(15,5))\n",
    "    ax[0].plot(model.history.history['val_f1_score'], label='val_f1_score')\n",
    "    ax[0].plot(model.history.history['f1_score'], label='f1_score')\n",
    "    ax[0].legend()\n",
    "    try:\n",
    "        ax[1].plot(model.history.history['val_accuracy'], label='val_accuracy')\n",
    "        ax[1].plot(model.history.history['accuracy'], label='accuracy')\n",
    "        ax[1].legend()\n",
    "    except:\n",
    "        ax[1].plot(model.history.history['val_precision_1'], label='val_precision_1')\n",
    "        ax[1].plot(model.history.history['precision_1'], label='precision_1')\n",
    "        ax[1].legend()\n",
    "    ax[2].plot(model.history.history['val_loss'], label='val_loss')\n",
    "    ax[2].plot(model.history.history['loss'], label='loss')\n",
    "    ax[2].legend()\n",
    "    fig.show;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1742156960338,
     "user": {
      "displayName": "Dimitri Caputo",
      "userId": "12075933984764029925"
     },
     "user_tz": -60
    },
    "id": "9CmLrebcYZ4f"
   },
   "outputs": [],
   "source": [
    "def get_analysis(model, testX, testy):\n",
    "    plot_learning_curves(model)\n",
    "    loss, acc, f1 = model.evaluate(testX, testy)\n",
    "    print(f'The model gave')\n",
    "    print(f'Loss: {loss:.2f}')\n",
    "    print(f'Accuracy: {acc:.2f}')\n",
    "    print(f'F1 Macro: {f1:.2f}')\n",
    "    y_pred = model.predict(testX)\n",
    "    y_res = np.argmax(y_pred, axis=1)\n",
    "    print(classification_report(testy, y_res))\n",
    "    return y_pred, y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1742156960342,
     "user": {
      "displayName": "Dimitri Caputo",
      "userId": "12075933984764029925"
     },
     "user_tz": -60
    },
    "id": "vSwR8z6vYZ4g"
   },
   "outputs": [],
   "source": [
    "def get_analysis_cat(model, testX, testy):\n",
    "    plot_learning_curves(model)\n",
    "    loss, acc, f1 = model.evaluate(testX, testy)\n",
    "    print(f'The model gave')\n",
    "    print(f'Loss: {loss:.2f}')\n",
    "    print(f'Accuracy: {acc:.2f}')\n",
    "    print(f'F1 Macro: {f1:.2f}')\n",
    "    predy = model.predict(testX)\n",
    "    resy = to_categorical(np.argmax(predy, axis=1))\n",
    "    print(classification_report(testy,resy))\n",
    "    return predy, resy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 86,
     "status": "ok",
     "timestamp": 1742156960431,
     "user": {
      "displayName": "Dimitri Caputo",
      "userId": "12075933984764029925"
     },
     "user_tz": -60
    },
    "id": "B6g22E4HYZ4h"
   },
   "outputs": [],
   "source": [
    "def compile_and_train(model, loss, opt, metrics, epochs, patience=None, steps=None):\n",
    "    model.compile(loss=loss,\n",
    "                optimizer=opt,\n",
    "                metrics=metrics)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    if patience != None:\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            epochs=epochs,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=[get_earlystopping(patience)],\n",
    "            steps_per_epoch=steps\n",
    "            )\n",
    "    else:\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            epochs=epochs,\n",
    "            validation_data=(X_val, y_val),\n",
    "            steps_per_epoch=steps\n",
    "            )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-89oUqvkJKb"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 394176,
     "status": "ok",
     "timestamp": 1741548874606,
     "user": {
      "displayName": "Dimitri Caputo",
      "userId": "12075933984764029925"
     },
     "user_tz": -60
    },
    "id": "nSGuYKWfjv1y",
    "outputId": "4ff9fcb9-d671-49ac-d1a2-a4d7bf59628f"
   },
   "outputs": [],
   "source": [
    "model_020 = Sequential([\n",
    "    Input(shape=(X_train.shape[1:])),\n",
    "    Rescaling(1./255),\n",
    "    RandomFlip('horizontal and vertical'),\n",
    "    RandomRotation(factor=(-0.3, 0.3)),\n",
    "    RandomTranslation(height_factor=(-0.3,0.3), width_factor=(-0.3, 0.3)),\n",
    "    RandomZoom(height_factor=(-0.3,0.3), width_factor=(-0.3, 0.3)),\n",
    "    Conv2D(filters, kernel_size=3, padding='SAME', activation='relu'),\n",
    "    Conv2D(filters, kernel_size=3, padding='SAME', activation='relu'),\n",
    "    Conv2D(filters, kernel_size=3, padding='SAME', activation='relu'),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Conv2D(filters*2, kernel_size=3, padding='SAME', activation='relu'),\n",
    "    Conv2D(filters*2, kernel_size=3, padding='SAME', activation='relu'),\n",
    "    Conv2D(filters*2, kernel_size=3, padding='SAME', activation='relu'),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Conv2D(filters*4, kernel_size=3, padding='SAME', activation='relu'),\n",
    "    Conv2D(filters*4, kernel_size=3, padding='SAME', activation='relu'),\n",
    "    Conv2D(filters*4, kernel_size=3, padding='SAME', activation='relu'),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Conv2D(filters*8, kernel_size=3, padding='SAME', activation='relu'),\n",
    "    Conv2D(filters*8, kernel_size=3, padding='SAME', activation='relu'),\n",
    "    Conv2D(filters*8, kernel_size=3, padding='SAME', activation='relu'),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(filters, activation='relu'),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "history020 = compile_and_train(model_020,\n",
    "                               loss=BinaryFocalCrossentropy(apply_class_balancing=True),\n",
    "                               opt='adam',\n",
    "                               metrics=[F1Score(average='macro')],\n",
    "                               epochs=200,\n",
    "                               patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2523,
     "status": "ok",
     "timestamp": 1741549218014,
     "user": {
      "displayName": "Dimitri Caputo",
      "userId": "12075933984764029925"
     },
     "user_tz": -60
    },
    "id": "hec7w51oqirk",
    "outputId": "7192e3b4-9b61-4c26-a7d7-b6bcd955eff7"
   },
   "outputs": [],
   "source": [
    "get_analysis_cat(model_020, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wR4PBIKcYZ5s"
   },
   "source": [
    "## Custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1742156920899,
     "user": {
      "displayName": "Dimitri Caputo",
      "userId": "12075933984764029925"
     },
     "user_tz": -60
    },
    "id": "OeHNIaretx4l"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.losses import CategoricalFocalCrossentropy\n",
    "from keras import backend as K\n",
    "from keras import ops\n",
    "\n",
    "def f1macro(y_true, y_pred):\n",
    "    tp = []\n",
    "    fp = []\n",
    "    fn = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "\n",
    "    cfc = CategoricalFocalCrossentropy()\n",
    "\n",
    "\n",
    "    for n in range(7):\n",
    "        tp.append(ops.sum(ops.cast(y_true[:, n] * y_pred[:, n], 'float32'), axis=0))\n",
    "        fp.append(ops.sum(ops.cast((1 - y_true[:, n]) * y_pred[:, n], 'float32'), axis=0))\n",
    "        fn.append(ops.sum(ops.cast(y_true[:, n] * (1 - y_pred[:, n]), 'float32'), axis=0))\n",
    "\n",
    "    for n in range(7):\n",
    "        precision.append(tp[n] / (tp[n] + fp[n] + K.epsilon()))\n",
    "        recall.append(tp[n] / (tp[n] + fn[n] + K.epsilon()))\n",
    "\n",
    "    for n in range(7):\n",
    "        f1.append(2 * (precision[n] * recall[n]) / (precision[n] + recall[n] + K.epsilon()))\n",
    "\n",
    "    f1macro = tf.stack(f1)\n",
    "    f1macroscore = ops.mean(f1macro)\n",
    "\n",
    "    f1macroscore = tf.where(tf.math.is_nan(f1macroscore), tf.zeros_like(f1macroscore), f1macroscore)\n",
    "\n",
    "    return cfc + (1 - f1macroscore)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "skin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
